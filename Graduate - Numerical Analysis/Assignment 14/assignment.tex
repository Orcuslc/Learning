%!TEX program = xelate
\input{structure.tex}
\usepackage{epstopdf}
\usepackage{graphics}
\usepackage{subfig}
\usepackage{listings}
\lstset{
  breaklines=true,
  xleftmargin=25pt,
  xrightmargin=25pt,
  aboveskip=0pt,
  belowskip=10pt,
  basicstyle=\ttfamily,
  showstringspaces=false,
  frame=ltrb,
  tabsize=4,
  numbers=left,
  numberstyle=\small,
  numbersep=8pt,
  morekeywords={*, factorial, sum, erlang},
  keywordstyle=\color{blue!70}, commentstyle=\color{red!50!green!50!blue!50},
}
\DeclareGraphicsExtensions{.eps,.ps,.jpg,.bmp}

\begin{document}

\title{Numerical Analysis \\ Assignment 13}
\date{\today}
\author{Chuan Lu}

\maketitle

\problem{1}{Problem 5.23}
\solution{(a)}{
Integrate (5.4.2) with respect to x on $[0, 1]$, we get from the convergence of the function series,
$$
\int_{0}^{1}(\frac{t}{e^t-1}e^{tx}-\frac{t}{e^t-1})dx = \sum_{j = 1}^{\infty}\frac{t^j}{j!}\int_{0}^{1}B_j(x)dx,
$$
thus
$$
1-\frac{t}{e^t-1} = \sum_{j=1}^{\infty}\frac{t^j}{j!}\int_{0}^{1}B_j(x)dx,
$$
with Taylor expansion of the left term of (5.4.5), we have $B_0 = 1$, thus
$$
\sum_{j=1}^{\infty}(B_j + \int_{0}^{1}B_j(x)dx)\frac{t^j}{j!} = 0.
$$
The left term can be regarded as a Taylor series of a function $f$ at $t = 0$ which satisfies $f(0) = 0$. By the uniqueness of Taylor expansion of a analytic function, we know $f\equiv 0$. Then for all $j > 0$,
$$
B_j + \int_{0}^{1}B_j(x)dx = 0.
$$
Now we know $B_0 = 1, ~B_1 = -\frac{1}{2}$, thus from definition (5.4.5),
$$
\frac{t}{e^t-1}+\frac{1}{2}t = 1+\sum_{j=2}^{\infty}B_j\frac{t^j}{j!} = \frac{t}{2}\frac{e^t+1}{e^t-1} \equiv g(t).
$$
We have
$$
g(-t) = -\frac{t}{2}\frac{e^{-t}+1}{e^{-t}-1} = -\frac{t}{2}\frac{1+e^t}{1-e^t} = g(t),
$$
hence $g(t)$ is an even function. Substitute $t = -t$ into last term,
$$
1+\sum_{j=2}^{\infty}(-1)^{j}B_j\frac{t^j}{j!} = g(-t) = g(t) = 1+\sum_{j=2}^{\infty}B_j\frac{t^j}{j!}.
$$
By simplification,
$$
\sum_{j = 1}^{\infty}B_{2j+1}\frac{t^{2j+1}}{(2j+1)!} = 0.
$$
With the same arguments and the uniqueness of Taylor expansion, we have
$$
B_{2j+1} = 0, ~\text{for all}~ j \ge 1.
$$
}
\solution{(b)}{
Take derivatives respective to $x$ on both sides of (5.4.2), 
$$
\frac{t^2}{e^t-1}e^{tx} = \sum_{j=1}^{\infty}B_j'(x)\frac{t^j}{j!}.
$$
Hence,
$$
\sum_{j=1}^{\infty}B_j'(x)\frac{t^j}{j!} = t\frac{t(e^{tx}-1)}{e^t-1} + \frac{t^2}{e^t-1} = \sum_{j=1}^{\infty}B_j(x)\frac{t^{j+1}}{j!} + \sum_{j=0}^{\infty}B_j\frac{t^{j+1}}{j!} = \sum_{j=2}^{\infty} jB_{j-1}(x)\frac{t^j}{j!} + \sum_{j=1}^{\infty}jB_{j-1}\frac{t^j}{j!}
$$
we have
$$
tB_1'(x) + \sum_{j=2}^{\infty}B_j'(x)\frac{t^j}{j!} = B_0t+\sum_{j=2}^{\infty}j(B_{j-1}(x)+B_{j-1})\frac{t^j}{j!}.
$$
Since $tB_1'(x) = B_0t$, with the uniqueness of Taylor expansion,
$$
B_j'(x) = j(B_{j-1}'(x)+B_{j-1}), ~\text{for all}~ j \ge 2.
$$
As proved in (a), $B_{2j+1} = 0 $ for all $j \ge 1$, the conclusion holds.
}

\problem{2}{Problem 5.31}
\lstinputlisting{romberg.m}
\lstinputlisting{prob2.m}
\lstinputlisting{res2.m}
\solution{Cmp}{
When comparing with Simpson's rule and trapezoidal rule, we can find that romberg extrapolation converges quicker than the corresponding order of the two rules, but slower than the $2^N $ order of the two rules.
}

\problem{3}{Problem 5.37}

\problem{4}{Problem 5.40}

\end{document}