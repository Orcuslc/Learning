% 6.5 下午2点预答辩
\documentclass[10pt]{beamer}

\usepackage{beamerthemesplit}
\usepackage{times}
\usefonttheme{structurebold}
\usepackage[english]{babel}
\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps}
\usepackage{amsmath,amssymb}
\usepackage[latin1]{inputenc}
\usepackage{multimedia}
\setcounter{equation}{0}
\numberwithin{equation}{section}
\setbeamercovered{dynamic}

\newcommand{\Lang}[1]{\operatorname{\text{\textsc{#1}}}}
\newcommand{\Class}[1]{\operatorname{\mathchoice
  {\text{\sf \small #1}}
  {\text{\sf \small #1}}
  {\text{\sf #1}}
  {\text{\sf #1}}}}

\newcommand{\NumSAT}      {\text{\small\#SAT}}
\newcommand{\NumA}        {\#_{\!A}}
\newcommand{\barA}        {\,\bar{\!A}}
\newcommand{\Nat}{\mathbb{N}}
\newcommand{\Set}[1]{\{#1\}}

\usepackage[latin1]{inputenc}
\usepackage{beamerthemesplit}
\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
\usepackage{graphicx}
\usepackage{subfloat}
\usepackage{chemarrow}
\usepackage{epsfig, subfigure}
\usepackage{latexsym,amssymb,lastpage}
\usepackage{graphicx,amsfonts}
\usepackage{times,mathptmx,bm,amsmath}
\usepackage{dcolumn}
\newcommand{\me}{\mathrm{e}}
\newcommand{\mi}{\mathrm{i}}
\newcommand{\md}{\mathrm{d}}

\usefonttheme{professionalfonts}
\usefonttheme{serif}

\def\mathfamilydefault{\rmdefault}

\usepackage{fontspec}
\setmainfont{Lucida Bright}

\newcommand{\nq}{\\[5pt]}
\newcommand{\nw}{\\[10pt]}

\setbeamercovered{transparent}
\usetheme{Madrid}
\pgfdeclareimage[height=.8cm]{logo}{fd1.eps}

\title[Statistics of Solutions to A Stochastic Differential Equation Set]{\huge Statistics of Solutions to A Stochastic Differential Equation Set}
\author[Chuan Lu]{\Large{Chuan Lu}}
\institute[ICS]{\large{Information and Computing Science}\linebreak \linebreak
\pgfuseimage{logo}
}
\date{June 2017}

\AtBeginSection[]{\frame{\frametitle{Outline}\tableofcontents[current]}}



\begin{document}

\frame{\titlepage}
\newcommand{\E}{\text{E}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Var}{\text{Var}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\s}{\mathbf{s}}
% \frame{\tableofcontents[all]}

\section{Introduction}
\subsection{SDE}
\frame{\frametitle{Introduction}
\begin{block}{The SDEs}
\[
\left\{
\begin{aligned}
& \frac{du(t)}{dt} = (-\gamma(t)+i\omega)u(t)+b(t)+f(t)+\sigma W(t), \nq
& \frac{db(t)}{dt} = (-\gamma_b+i\omega_b)(b(t)-\hat{b})+\sigma_b W_b(t), \nq
& \frac{d\gamma(t)}{dt} = -d_\gamma(\gamma(t)-\hat{\gamma})+\sigma_\gamma W_\gamma(t)
\end{aligned}
\right.
\]
\end{block}
The initial values are complex random variables, with their first-order and second-order statistics known. }

\frame{\frametitle{Introduction}
\begin{block}{Solution}
With knowledge of ODEs, the solution of the SDE set is
\[
\left\{
\begin{aligned}
& b(t) = \hat{b}+(b_0-\hat{b})e^{\lambda_b(t-t_0)}+\sigma_b\int_{t_0}^t e^{\lambda_b(t-s)}dW_b(s) \nq
& \gamma(t) = \hat{\gamma}+(\gamma_0-\hat{\gamma})e^{-d_{\gamma}(t-t_0)}+\sigma_{\gamma}\int_{t_0}^t e^{-d_{\gamma}(t-s)}dW_{\gamma}(s) \nq
& u(t) = e^{-J(t_0, t)+\hat{\lambda}(t-t_0)}u_0 + \int_{t_0}^t (b(s)+f(s))e^{-J(s, t)+\hat{\lambda}(s-t_0)}ds \nq
&\quad\quad + \sigma\int_{t_0}^{t}e^{-J(s, t)+\hat{\lambda}(s-t_0)}dW(s)
\end{aligned}
\right.
\]
\end{block}
with $\lambda_b = -\gamma_b+i\omega_b$, ~$\hat{\lambda} = -\hat{\gamma}+i\omega$, ~$J(s, t) = \int_s^t (\gamma(s')-\hat{\gamma})ds'$.
}

\subsection{It\^o Integration}
\frame{\frametitle{It\^o integration}
\begin{block}{It\^o Isometry}
$\forall f\in\mathcal{V}(S, T)$, $B_t$ is a standard Brownian motion,
$$\E\left[\left(\int_S^T f(t, \omega)dB_t\right)^2\right] = \E\left[\int_S^T f^2(t, \omega)dt\right].$$
\end{block}	
\begin{block}{It\^o Formula}
Assume that $X_t$ is a It\^{o} process satisfying
$dX_t = udt+vdB_t,$
$g(t, x) \in C^2\left([0, \infty) \times \mathbb{R}\right)$, then $Y_t = g(t, X_t)$ is also a It\^{o} process satisfying
$$dY_t = \frac{\partial g}{\partial t}(t, X_t)dt + \frac{\partial g}{\partial x}(t, X_t)dX_t + \frac{1}{2}\frac{\partial^2 g}{\partial x^2}(t, X_t)\cdot(dX_t)^2,$$
with
$dt\cdot dt = dt\cdot dB_t = dB_t\cdot dt = 0, ~dB_t\cdot dB_t = dt.$
\end{block}
}

\frame{\frametitle{It\^o integration}
\begin{block}{Linear property of It\^o integration}
$$
\begin{aligned}
& (1)\quad \int_S^T fdB_t = \int_S^U fdB_t + \int_U^T fdB_t, ~\text{a.e.}\nq
& (2)\quad \int_S^T (cf+g)dB_t = c\int_S^T fdB_t + \int_S^T gdB_t, ~\text{a.e.}\nq
& (3)\quad \E\left[\int_S^T fdB_t\right] = 0.
\end{aligned}
$$
\end{block}
}

\section{Statistics of $b(t)$ and $\gamma(t)$}
\subsection{Mean}
\frame{\frametitle{Mean}
\quad With property of It\^o integration (3), it is easy to know
\begin{block}{}
$$
\begin{aligned}
&\E(b(t)) = \hat{b} + (\E[b_0] - \hat{b})e^{\lambda_b(t-t_0)} \nq
&\E(\gamma(t)) = \hat{\gamma} + (\E[\gamma_0] - \hat{\gamma})e^{-d_{\gamma}(t-t_0)}
\end{aligned}
$$
\end{block}
}

\subsection{Variance}
\frame{\frametitle{Variance}
\quad According to definition, 
\[
\begin{aligned}
\text{Var}&(b(t)) = \E\left[(b(t)-\E[b(t)])(b(t)-\E[b(t)])^*\right] \nq
&= e^{-2\gamma_b(t-t_0)}\Var(b_0) + \E\left[\sigma_b^2\int_{t_0}^t e^{\lambda_b(t-s)}dW_b(s)\left(\int_{t_0}^t e^{\lambda_b(t-s)}dW_b(s)\right)^*\right] \nq
&= e^{-2\gamma_b(t-t_0)}\Var(b_0) + \sigma_b^2\E\left[\int_{t_0}^t e^{-2\gamma_b(t-s)}ds\right]
\end{aligned}
\]
\quad The last step takes advantage of It\^o isometry.
\begin{block}{}
\[
\begin{aligned}
&\Var(b(t)) = e^{-2\gamma_b(t-t_0)}\Var(b_0) + \frac{\sigma_b^2}{2\gamma_b}(1-e^{-2\gamma_b(t-t_0)}) \nq
&\Var(\gamma(t)) = e^{-2d_\gamma(t-t_0)}\Var(\gamma_0) + \frac{\sigma_\gamma^2}{2d_\gamma}(1-e^{-2d_\gamma(t-t_0)})
\end{aligned}
\]
\end{block}
}

\subsection{Covariance}
\frame{\frametitle{Covariance}
\[
\begin{aligned}
\Cov&(b(t), b(t)^*) = \E\left[(b(t) - \E[b(t)])(b(t)^* - \E[b(t)^*])\right] \nw
&= \E\left[(b_0 - \E[b_0])(b_0^* - \E[b_0^*])e^{2\lambda_b (t-t_0)}\right] + \sigma_b\E\left[(b_0 - \E[b_0])\int_{t_0}^{t}e^{\lambda_b(t-s)}dW_b(s)\right]\nq
&+ \sigma_b\E\left[(b_0^*-\E[b_0^*])e^{\lambda_b(t-t_0)}\int_{t_0}^{t}e^{\lambda_b(t-s)}dW_b(s)\right] + \sigma_b^2\E\left[(\int_{t_0}^{t}e^{\lambda_b(t-s)}dW_b(s))^2\right]
\end{aligned}
\]
\quad With property of It\^o integration (3), the second and third term are both 0; with It\^o isometry we know the last term is also 0.
\begin{block}{}
\[
\begin{aligned}
&\Cov(b(t), b(t)^*) = \E[(b_0-\E[b_0])(b_0^*-\E[b_0^*])]e^{2\lambda_b(t-t_0)} = \Cov(b_0, b_0^*)e^{2\lambda_b(t-t_0)} \nq
&\Cov(b(t), \gamma(t)) = \E[(b(t) - \E[b(t)])(\gamma(t)-\E[\gamma(t)])]= \Cov(b_0, \gamma_0)e^{(\lambda_b-d_\gamma)(t-t_0)}
\end{aligned}
\]
\end{block}
}

\section{Statistics of $u(t)$}
\subsection{Mean}
\frame{\frametitle{Mean}
\quad Using the same properties, it's obvious to have
\[
\begin{aligned}
\E[u(t)] &= e^{\hat{\lambda}(t-t_0)}\E\left[e^{-J_0(t_0, t)}u_0\right] + \int_{t_0}^t e^{\hat{\lambda}(t-s)}\E\left[b(s)e^{-J(s, t)}\right]ds  \nq
&+ \sigma\int_{t_0}^{t} e^{\hat{\lambda}(t-s)}f(s)\E\left[e^{-J(s, t)}\right]ds.
\end{aligned}
\]
\quad It is necessary to compute expectations of terms like
$
\E[ze^{bx}],
$
where $z$ is a complex-valued Gaussian random variable and x is a real-valued Gaussian variable. We propose two lemmas here.
}

\frame{\frametitle{Lemma 1}
\begin{lemma}\label{lemma 1}
$$\E\left[ze^{ibx}\right] = (\E[z]+ib\Cov(z, x))e^{ib\E[x]-\frac{1}{2}b^2\Var(x)}$$
with $z$ being a complex-valued Gaussian, and $x$ a real-valued Gaussian.
\end{lemma}

\begin{Corollary} \label{corollary 1}
Under the condition of Lemma \ref{lemma 1},
$$\E\left[ze^{bx}\right] = (\E[z]+b\Cov(z, x))e^{b\E[x]+\frac{1}{2}b^2\Var(x)}.$$
\end{Corollary}
Proof of lemma \ref{lemma 1} takes advantage of the characteristic function of multivariable Gaussian distribution.
}

\frame{\frametitle{Proof}

\quad Let $z = y+iw, \quad y, w \in \mathbb{R}.$ Denote $\v = (x, y, w)$, then $\v$ satisfies the multivariable Gaussian distribution, with its characteristic function
\[
\phi_\v(\s) = \exp(i\s^\top\E[\v]-\frac{1}{2}\s^\top\Sigma\s).
\]
\quad Let $g(\v)$ being the PDF of $\v$, then one knows from that char. func. being Fourier transform of PDF,
\[
\phi_\v(\s) = \frac{1}{(2\pi)^3}\int e^{i\s^\top\v}g(\v)d\v
\]
\quad According to the differential property of Fourier transform, 
\[
\frac{\partial \phi_\mathbf{v}(\mathbf{s})}{\partial s_2} = \frac{1}{(2\pi)^3}\int iy_0e^{i\mathbf{s}^\top\mathbf{v}}g(\mathbf{v})d\mathbf{v} = i\E\left[y_0e^{i\mathbf{s}^\top\mathbf{v}}\right].
\]
\quad Let $\v = (b, 0, 0)^\top$, 
\[
\begin{aligned}
&\left.\E[y_0e^{ibx_0}] = -i\frac{\partial \phi_\mathbf{v}(s)}{\partial s_2}\right|_{\mathbf{s} = (b, 0, 0)^\top} \\
&\left.\E[y_0e^{ibx_0}] = -i\frac{\partial \phi_\mathbf{v}(s)}{\partial s_2}\right|_{\mathbf{s} = (b, 0, 0)^\top}
\end{aligned}
\]
}

\frame{\frametitle{Proof}
\quad From PDF of multivariable Gaussian distribution, one knows
\[
\begin{aligned}
&\frac{\partial \phi_\mathbf{v}(\mathbf{s})}{\partial s_2} = (i\E[y_0]-\Var(y_0)s_2-\Cov(x_0, y_0)s_1-\Cov(y_0, w_0)s_3)\phi_{\mathbf{v}}(s) \nq
&\frac{\partial \phi_\mathbf{v}(\mathbf{s})}{\partial s_3} = (i\E[w_0]-\Var(w_0)s_3-\Cov(x_0, w_0)s_1-\Cov(y_0, w_0)s_2)\phi_{\mathbf{v}}(s)
\end{aligned}
\]
\quad Compute the partial derivatives at $\s = (b, 0, 0)^\top$,
\[
\begin{aligned}
&\E\left[y_0e^{ibx_0}\right] = (\E[y_0]+i\Cov(x_0, y_0)b)\exp(ib\E[x_0] - \frac{1}{2}\Var(x_0)b^2) \nq
&\E\left[w_0e^{ibx_0}\right] = (\E[w_0]+i\Cov(x_0, w_0)b)\exp(ib\E[x_0] - \frac{1}{2}\Var(x_0)b^2)
\end{aligned}
\]
\quad Then $$\E\left[ze^{ibx}\right] = (\E[z]+ib\Cov(z, x))e^{ib\E[x]-\frac{1}{2}b^2\Var(x)}.$$ \qed
}

\frame{\frametitle{Lemma 2}
\begin{lemma} \label{lemma 2}
\[
\begin{aligned}
\E\left[zwe^{bx}\right] &= \left[\E[z]\E[w]+\Cov(z, w^*)+b(\E[z]\Cov(w, x))+\E[w]\Cov(z, x)+ \right. \nq
& \left.b^2\Cov(z, x)\Cov(w, x)\right]e^{b\E[x]+\frac{b^2}{2}\Var(x)}. \\
\end{aligned}
\]
with $z, w$ being complex-valued Gaussian, and $x$ real-valued Gaussian.
\end{lemma}
The proof of this lemma is the same as Lemma \ref{lemma 1}.
}

\frame{\frametitle{Mean}
\quad We now make use of Lemma \ref{lemma 1} to obtain the mean of $u(t)$.
\begin{block}{}
\[
\begin{aligned}
&\E[u(t)] = e^{\hat{\lambda}(t-t_0)}(\E[u_0]-\Cov(u_0, J(t_0, t)))e^{-\E[J(t_0, t)]+\frac{1}{2}\Var(J(t_0, t))} \nw
&\quad\quad+ \int_{t_0}^t e^{\hat{\lambda}(t-s)}(\hat{b}+e^{\lambda_b(s-t_0)}(\E[b_0]-\hat{b}-\Cov(b_0, J(s, t))))e^{-\E[J(s, t)]+\frac{1}{2}\Var(J(s, t))}ds \nq
&\quad\quad+ \int_{t_0}^t e^{\hat{\lambda}(t-s)}f(s)e^{-\E[J(s, t)]+\frac{1}{2}\Var(J(s, t))}ds
\end{aligned}
\]
\end{block}
The terms $\Cov(u_0, J(s, t))$, $\Cov(b_0, J(s, t))$, $\E[J(s, t)]$ and $\Var(J(s, t))$ can be found using It\^o isometry.
}

\subsection{Variance}
\frame{\frametitle{Variance}
\quad Denote $u(t) = A + B + C$, 
\[\left\{
\begin{split}
&A = e^{-J(t_0, t)+\hat{\lambda}(t-t_0)}u_0, \nq
&B = \int_{t_0}^t (b(s)+f(s))e^{-J(s, t)+\hat{\lambda}(t-s)}ds, \nq
&C = \sigma\int_{t_0}^t e^{-J(s, t)+\hat{\lambda}(t-s)}dW(s).
\end{split}
\right.
\]
\quad By definition we find $\Var(u(t)) = \E\left[|u(t)|^2\right] - \left|\E[u(t)]\right|^2$, with
\[
\E\left[|u(t)|^2\right] = \E\left[|A|^2\right]+ \E\left[|B|^2\right] + \E\left[|C|^2\right] + 2\text{Re}\{\E[A^*B]\}.
\]
\quad We can obtain $\E\left[|A|^2\right]$ by Lemma 2, and $\E\left[|B|^2\right]$ by It\^o isometry and noticing that
\[
\Cov(J(s, t), J(r, t)) = \Var(J(s, t)) + \Cov(J(s, t), J(r, s)).
\]
\quad $\E\left[|C|^2\right]$ and $\text{Re}\{\E[A^*B]\}$ can also be computed by It\^o isometry and property of It\^o integration.
}

\subsection{Covariance}
\frame{\frametitle{Covariance}
\quad By definition, 
\[
\begin{aligned}
&\Cov(u(t), u^*(t)) = \E\left[u(t)^2\right]-\E[u(t)]^2 \nq
&\Cov(u(t), \gamma(t)) = \E[u(t)(\gamma(t)-\hat\gamma)] + \E[u(t)](\hat\gamma-\E[\gamma(t)]) \nq
&\Cov(u(t), b(t)) = \E[u(t)b^*(t)] - \E[u(t)]\E[b(t)]^* \nq
&\Cov(u(t), b^*(t)) = \E[u(t)b(t)] - \E[u(t)]\E[b(t)].
\end{aligned}
\]
\quad Each term can be obtained by Lemma 3 and It\^o isometry.
}

\section{Numerical Simulation}
\frame{\frametitle{Parameters}
\quad External forcing $f(t) = \frac{3}{2}e^{0.1it}$, and parameters of the equation set are given
\[
\left\{
\begin{aligned}
&d = 1.5,  &d_\gamma = 0.01d\\
&\sigma = 0.1549,& \omega = 1.78 \\
&\sigma_\gamma = 5\sigma, &\gamma_b = 0.1d \\
&\sigma_b = 5\sigma, &\omega_b = \omega \\
&\hat{b} = 0, &\hat\gamma = 0
\end{aligned}
\right.
\]
\quad We assume the initial values satisfying
\[
\left\{
\begin{aligned}
&\text{Re}(u_0), \text{Im}(u_0) \sim \mathcal{N}(0, \frac{1}{\sqrt{2}}), \quad \text{i.i.d.} \nq
&\text{Re}(b_0), \text{Im}(b_0) \sim \mathcal{N}(0, \frac{1}{\sqrt{2}}), \quad \text{i.i.d.} \nq
&\gamma_0 \sim \mathcal{N}(0, 1) \quad \text{i.i.d.}
\end{aligned}
\right.
\]
}

\frame{\frametitle{Parameters}
\quad The statistics between initial values are
\[
\left\{
\begin{aligned}
&\Cov(u_0, u_0^*) = 0 \nq
&\Cov(u_0, \gamma_0) = 0 \nq
&\Cov(u_0, b_0) = 0 \nq
&\Cov(u_0, b_0^*) = 0
\end{aligned}
\right.
\]
}

\frame{\frametitle{Euler-Maruyama Scheme}
\quad It\^o integration can be simulated by E-M scheme:
\[
X_j = X_{j-1}+f(X_{j-1})\Delta t+g(X_{t-1})(W(\tau_j) - W(\tau_{j-1})),
\]
\quad with 
\[
W(\tau_j) - W(\tau_{j-1}) = \sum_{k = jR-R+1}^{jR} dW_k,
\]
\quad and $R$ being the step length of E-M scheme,
\[
dW = \sqrt{\Delta t}*\tt{randn()}.
\]
}

\frame{\frametitle{Simulation}
\quad Simulate 100,000 times with $R = 1$, the results are as follows:
\begin{figure}[!ht]
\centering
\subfigure[\text{E}(Re(u))]{
  \includegraphics[width = .2\textwidth]{../NumericalSimulation/res/EuRe.png}
  }\hfill
\subfigure[E(Im(u))]{
  \includegraphics[width = .2\textwidth]{../NumericalSimulation/res/EuIm.png}
  }\hfill
\subfigure[E(Re(b))]{
  \includegraphics[width = .2\textwidth]{../NumericalSimulation/res/EbRe.png}
  }\hfill
\subfigure[E(Im(b))]{
  \includegraphics[width = .2\textwidth]{../NumericalSimulation/res/EbIm.png}
  }\\
\subfigure[E($\gamma$)]{
  \includegraphics[width = .2\textwidth]{../NumericalSimulation/res/Egamma.png}
  }\hfill
\subfigure[Var(u)]{
  \includegraphics[width = .2\textwidth]{../NumericalSimulation/res/Varu.png}
  }\hfill
\subfigure[Var(b)]{
  \includegraphics[width = .2\textwidth]{../NumericalSimulation/res/Varb.png}
  }\hfill
\subfigure[Var($\gamma$)]{
  \includegraphics[width = .2\textwidth]{../NumericalSimulation/res/Vargamma.png}
  }
\caption{Simulation of Expectations and Variances}
\label{Simulation of Expectations and Variances}
\end{figure}
}

\frame{\frametitle{Simulation}
\begin{figure}[!ht]
\centering
\subfigure[$\text{Re}(\Cov(u, u^*))$]{
  \includegraphics[width = .2\textwidth]{../NumericalSimulation/res/CovuustarRe.png}
}\hfill
\subfigure[$\text{Im}(\Cov(u, u^*))$]{
  \includegraphics[width = .2\textwidth]{../NumericalSimulation/res/CovuustarIm.png}
}\hfill
\subfigure[$\text{Re}(\Cov(u, \gamma))$]{
  \includegraphics[width = .2\textwidth]{../NumericalSimulation/res/CovugammaRe.png}
}\hfill
\subfigure[$\text{Im}(\Cov(u, \gamma))$]{
  \includegraphics[width = .2\textwidth]{../NumericalSimulation/res/CovugammaIm.png}
}\\
\subfigure[$\text{Re}(\Cov(u, b))$]{
  \includegraphics[width = .2\textwidth]{../NumericalSimulation/res/CovubRe.png}
}\hfill
\subfigure[$\text{Im}(\Cov(u, b))$]{
  \includegraphics[width = .2\textwidth]{../NumericalSimulation/res/CovubIm.png}
}\hfill
\subfigure[$\text{Re}(\Cov(u, b^*))$]{
  \includegraphics[width = .2\textwidth]{../NumericalSimulation/res/CovubstarRe.png}
}\hfill
\subfigure[$\text{Im}(\Cov(u, b^*))$]{
  \includegraphics[width = .2\textwidth]{../NumericalSimulation/res/CovubstarIm.png}
}
\caption{Simulation of Covariances}
\label{Simulation of Covariances}
\end{figure}
}

\frame{\frametitle{Discussion}
From the simulation results we find that the simulation of expectations and variances fit the theoretical results properly; but the simulation of covariances have an error of $O(10^{-3})$. It could be caused by the condition number of multiplication of complex numbers.
}

\frame{
	\quad\quad\centering{ \Huge Thank you!}
}
\end{document}
