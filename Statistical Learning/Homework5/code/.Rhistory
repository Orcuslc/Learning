######### Gibbs Sampling for problem 3 ###########
sample3 = function(param1, param2) {
lambda = 1/(param1 + param2 + 1);
##### Using inverse transform algorithm to generate the distribution:
##### f(x|lambda) = lambda*exp(-lambda*x);
u = runif(1);
x = -lambda*log(lambda*u);
return(x);
}
gibbs_sampling3 = function(n, init_param) {
##### Initialize parameters;
x = rep(0, n);
y = rep(0, n);
z = rep(0, n);
x[1] = init_param[1];
y[1] = init_param[2];
z[1] = init_param[3];
##### Iterative Sampling;
for(i in 2:n) {
x[i] = sample3(y[i-1], z[i-1]);
y[i] = sample3(x[i], z[i-1]);
z[i] = sample3(x[i], y[i]);
}
rlist = list(x, y, z);
return(rlist);
}
estimate3 = function(n) {
res = gibbs_sampling3(2*n, c(1, 1, 1));
X = res[[1]];
Y = res[[2]];
Z = res[[3]];
X_sample = X[(n+1):(2*n)];
Y_sample = Y[(n+1):(2*n)];
Z_sample = Z[(n+1):(2*n)];
sum = sum(X_sample)*sum(Y_sample)*sum(Z_sample);
return(sum/(n^3));
}
estimate3(100000)
is.na(2)
a = c(1, na,2)
a = c(1, na,2)
a = c(1, na,2)
a = c(1, NA,2)
a
is.na(a)
which(is.na(a))
a = a[which(is.na(a))]
a
a = c(1, NA,2)
a = a[-which(is.na(a))]
a
all.equal(1, 2)
all.equal(1, 2,2)
all.equal(1, 2,4)
all.equal(1, 2,1,2)
as.qr(matrix(c(1,5,2,3), ncol=2))
matrix(c(1,5,2,3), ncol = 2)
a = matrix(c(1,5,2,3), ncol = 2)
as.qr(a)
help(as.qr)
c, d = as.qr(a)
(c, d) = as.qr(a)
c = as.qr(a)
cat
hep(cat)
help("cat")
for (i in seq(1, 9, by=2))
;
for (i in seq(1, 9, by=2)) {}
for (i in seq(1, 9, by=2)) {
print(i)
}
for (i in seq(1, 9, by=2)) {
cat(i)}
count = 0
for (i in seq(1, 9, by=2)) {}
for (i in seq(1, 9, by=2)) {
count = count + i
cat(count, sep='\n')
}
help(function)
t1 = function(a, b, c) {}
t1 = function(a, b, c) {
delta = b^2 - 4*a*c;
if (delta >= 0) {
return TRUE;
adasd
dasdasdasdasdasdadasdasdas
dasdasdasdasdsadsadsadsad
dassssssssssssssssssssssssssssssssssss=1
1
dassssssssssssssssssssssssssssssssssss
concentrate
##### Gibbs Sampling for Problem 4 #####
sample4_x = function(y, n) {
probs = rep(0, n+1);
c = 1; # C_{n}^{x};
ycon = 1; # y^x * (1-y)^(nx);
for(i in 1:(n+1)) {
probs[i] = c * ycon;
c = c*(n-i)/(i+1);
ycon = ycon * y * (1-y)^n;
}
normalized_probs = rep(0, n+1);
sums = 0;
total_sum = sum(probs);
u = runif(1);
for(i in 1:(n+1)) {
sums = sums + probs[i];
normalized_probs[i] = sums/total_sum;
if(u < normalized_probs[i]) {
x = i-1;
break;
}
}
return(x);
}
sample4_y = function(x, n, alpha, beta) {
y = rbeta(1, x+alpha, n*x+beta);
return(y);
}
sample4_n = function(x, y, lambda) {
### Use Acceptance-Rejection Method to generate N ###
if(x == 0){
n_max = 0;
}
else {
n_max = floor(x/(1-(1-y)^x));
}
c = choose(n_max, x)*(1-y)^(n*x);
# print(c(n_max, x, y, n));
while(1) {
pois = -1;
while(pois < x) {
pois = rpois(1, lambda);
}
u = runif(1);
if(u*c < choose(pois, x)*(1-y)^(pois*x)) {
return(pois);
}
}
}
gibbs_sampling4 = function(n, alpha, beta, lambda, init_param) {
x = rep(0, n);
y = rep(0, n);
n = rep(0, n);
x[1] = init_param[1];
y[1] = init_param[2];
n[1] = init_param[3];
for(i in 2:n) {
print(i)
x[i] = sample4_x(y[i-1], n[i-1]);
y[i] = sample4_y(x[i], n[i-1], alpha, beta);
n[i] = sample4_n(x[i], y[i], lambda);
}
rlist = list(x, y, n);
return(rlist);
}
estimate4 = function(n, alpha, beta, lambda) {
rl = gibbs_sampling4(4*n, alpha, beta, lambda, c(2, 0.5, 3));
x = rl[[1]];
y = rl[[2]];
n = rl[[3]];
ex = mean(x[(3*n):(4*n)]);
ey = mean(y[(3*n):(4*n)]);
en = mean(n[(3*n):(4*n)]);
return(c(ex, ey, en));
}
n = 10000
alpha = 2
beta = 3
lambda = 4
estimate4(n, alpha, beta, lambda)
install.packages(ggplot2)
ggplot2
install.packages(ggplot2)
install.packages(qgplot2)
install.packages(gqplot2)
setwd("~/GitHub/Learning/Statistical Learning/Homework5/code")
n = 10
index1 = -1:n
index2 = rep(1:5. ceiling(2))[1:10]
index2 = rep(1:5, ceiling(2))[1:10]
index2 = sample(index2, n)
index2
source('Best_Subset.R')
source('Lasso.R')
source('LARS.R')
source('PCR.R')
source('PLS.R')
source('Ridge.R')
source('Cross_Validation.R')
source('Read.R')
data = read('../data/test3.csv');
X = data$X;
Y = data$Y;
k = 10; # 10-fold Cross Validation
### Best Subset ###
index = Partition(X, Y, k);
print(index)
a, n = 1, 2
c(a, n) = c(1, 2)
source('Best_Subset.R')
source('Lasso.R')
source('LARS.R')
source('PCR.R')
source('PLS.R')
source('Ridge.R')
source('Cross_Validation.R')
source('Read.R')
source('std.R')
data = read('../data/test3.csv');
X = data$X;
Y = data$Y;
k = 10; # 10-fold Cross Validation
### Best Subset ###
index = Partition(X, Y, k);
error = 0;
for(i in 1:k) {
sets = CV(X, Y, index, k, i);
beta = Best_Subset(sets$X1, sets$Y1)$beta;
error = error + calc_err(sets$X2, sets$Y2, beta);
}
error = error/k;
print(error);
print(error);
source('Best_Subset.R')
source('Lasso.R')
source('LARS.R')
source('PCR.R')
source('PLS.R')
source('Ridge.R')
source('Cross_Validation.R')
source('Read.R')
source('std.R')
data = read('../data/test3.csv');
X = data$X;
Y = data$Y;
k = 10; # 10-fold Cross Validation
### Best Subset ###
index = Partition(X, Y, k);
error = 0;
for(i in 1:k) {
sets = CV(X, Y, index, k, i);
beta = Best_Subset(sets$X1, sets$Y1, k = (nrow(sets$X1)-1))$beta;
error = error + calc_err(sets$X2, sets$Y2, beta);
}
error = error/k;
print(error);
source('Best_Subset.R')
source('Lasso.R')
source('LARS.R')
source('PCR.R')
source('PLS.R')
source('Ridge.R')
source('Cross_Validation.R')
source('Read.R')
source('std.R')
data = read('../data/test3.csv');
X = data$X;
Y = data$Y;
k = 10; # 10-fold Cross Validation
### Best Subset ###
index = Partition(X, Y, k);
error = 0;
for(i in 1:k) {
sets = CV(X, Y, index, k, i);
beta = Best_Subset(sets$X1, sets$Y1, k = (nrow(sets$X1)-1))$beta;
print(beta)
error = error + calc_err(sets$X2, sets$Y2, beta);
}
error = error/k;
print(error);
source('Best_Subset.R')
source('Lasso.R')
source('LARS.R')
source('PCR.R')
source('PLS.R')
source('Ridge.R')
source('Cross_Validation.R')
source('Read.R')
source('std.R')
data = read('../data/test3.csv');
X = data$X;
Y = data$Y;
k = 10; # 10-fold Cross Validation
### Best Subset ###
index = Partition(X, Y, k);
error = 0;
for(i in 1:k) {
sets = CV(X, Y, index, k, i);
beta = Best_Subset(sets$X1, sets$Y1, k = (nrow(sets$X1)-1))$beta;
print(beta)
error = error + calc_err(sets$X2, sets$Y2, beta);
}
error = error/k;
print(error);
source('Best_Subset.R')
source('Lasso.R')
source('LARS.R')
source('PCR.R')
source('PLS.R')
source('Ridge.R')
source('Cross_Validation.R')
source('Read.R')
source('std.R')
data = read('../data/test3.csv');
X = data$X;
Y = data$Y;
k = 10; # 10-fold Cross Validation
### Best Subset ###
index = Partition(X, Y, k);
error = 0;
for(i in 1:k) {
sets = CV(X, Y, index, k, i);
beta = Best_Subset(sets$X1, sets$Y1, k = (nrow(sets$X1)-1))$beta;
print(beta)
error = error + calc_err(sets$X2, sets$Y2, beta);
}
error = error/k;
print(error);
source('Best_Subset.R')
source('Lasso.R')
source('LARS.R')
source('PCR.R')
source('PLS.R')
source('Ridge.R')
source('Cross_Validation.R')
source('Read.R')
source('std.R')
data = read('../data/test3.csv');
X = data$X;
Y = data$Y;
k = 10; # 10-fold Cross Validation
### Best Subset ###
index = Partition(X, Y, k);
error = 0;
for(i in 1:k) {
sets = CV(X, Y, index, k, i);
beta = Best_Subset(sets$X1, sets$Y1, k = (nrow(sets$X1)-1))$beta;
print(beta)
error = error + calc_err(sets$X2, sets$Y2, beta);
}
error = error/k;
print(error);
